{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SUPPORT VECTOR CLASSIFIER** üèõÔ∏è\n",
    "\n",
    "this script is for setup, execution, and evaluation of the support vector classifier algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import funcs as f\n",
    "import scipy.stats as sts\n",
    "from sklearn.svm import SVC\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set clf, import data from data_organization.ipynb, and set random seed\n",
    "'''\n",
    "clf = SVC(kernel='poly', C=.1, gamma=10)\n",
    "\n",
    "# final df for display\n",
    "dfr = pd.DataFrame(index=['All', 'GUS Focus', 'TGUS Focus', 'Raw Focus'], columns=['Avg. Train %', 'Avg. Validation %', 'Avg. Test %',\n",
    "                                                                                   'Best Sc. Train %', 'Best Sc. Validation %', 'Best Sc. Test %'])\n",
    "\n",
    "df_all = pd.read_csv('../data/df_all.csv').drop('Unnamed: 0', axis = 1)\n",
    "df_gus = pd.read_csv('../data/df_gus.csv').drop('Unnamed: 0', axis = 1)\n",
    "df_tgus = pd.read_csv('../data/df_tgus.csv').drop('Unnamed: 0', axis = 1)\n",
    "df_tgus_st = pd.read_csv('../data/df_tgus*.csv').drop('Unnamed: 0', axis = 1)\n",
    "df_raw = pd.read_csv('../data/df_raw.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/31/n7d1lkjj6y1_lcp76m8nrtbh0000gn/T/ipykernel_64986/3406437178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mall_tests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_crossval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mall_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mall_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/orie 4741/orie4741-final/scripts/funcs.py\u001b[0m in \u001b[0;36mkfold_crossval\u001b[0;34m(df, clf, modelName)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# assess model accuracy on train/validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mtrain_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_assessment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mavg_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/orie 4741/orie4741-final/scripts/funcs.py\u001b[0m in \u001b[0;36mmodel_assessment\u001b[0;34m(modelName, clf, xt, yt, xv, yv)\u001b[0m\n\u001b[1;32m    143\u001b[0m           \u001b[0myv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     '''\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Considering GUS, TGUS,  TGUS*, and raw values with other features\n",
    "\n",
    "'''\n",
    "all_train =[]\n",
    "all_vals =[]\n",
    "all_tests=[]\n",
    "for i in range(100):\n",
    "    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_all, clf, 'svc')\n",
    "    all_train = np.append(train_acc, all_train)\n",
    "    all_vals = np.append(val_acc, all_vals)\n",
    "    all_tests = np.append(test_acc, all_tests)\n",
    "\n",
    "    # keep best model\n",
    "    if test_acc >= np.max(all_tests):\n",
    "        all_model = model\n",
    "        all_train_best = train_acc\n",
    "        all_val_best = val_acc\n",
    "        all_test_best = test_acc\n",
    "\n",
    "# get average accuracies\n",
    "all_train_avg = round(np.mean(all_train),1)\n",
    "all_vals_avg = round(np.mean(all_vals),1)\n",
    "all_tests_avg = round(np.mean(all_tests),1)\n",
    "\n",
    "print('All - Results:')\n",
    "print(f'Best Scenario Training Accuracy: {all_train_best}%')\n",
    "print(f'Average Training Accuracy: {all_train_avg}%')\n",
    "print(f'Best Scenario Validation Accuracy: {all_val_best}%')\n",
    "print(f'Average Validation Accuracy: {all_vals_avg}%')\n",
    "print(f'Best Scenario Test Accuracy: {all_test_best}%')\n",
    "print(f'Average Test Accuracy: {all_tests_avg}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tests[all_tests == 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUS - Results:\n",
      "Best Scenario Training Accuracy: 94.6%\n",
      "Average Training Accuracy: 96.1%\n",
      "Best Scenario Validation Accuracy: 97.1%\n",
      "Average Validation Accuracy: 96.6%\n",
      "Best Scenario Test Accuracy: 99.1%\n",
      "Average Test Accuracy: 93.4%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Considering just GUS with other features\n",
    "\n",
    "'''\n",
    "gus_train =[]\n",
    "gus_vals =[]\n",
    "gus_tests=[]\n",
    "for i in range(100):\n",
    "    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_gus, clf, 'svc')\n",
    "    gus_train = np.append(train_acc, gus_train)\n",
    "    gus_vals = np.append(val_acc, gus_vals)\n",
    "    gus_tests = np.append(test_acc, gus_tests)\n",
    "\n",
    "    # keep best model\n",
    "    if test_acc >= np.max(gus_tests):\n",
    "        gus_model = model\n",
    "        gus_train_best = train_acc\n",
    "        gus_val_best = val_acc\n",
    "        gus_test_best = test_acc\n",
    "\n",
    "# get average accuracies\n",
    "gus_train_avg = round(np.mean(gus_train),1)\n",
    "gus_vals_avg = round(np.mean(gus_vals),1)\n",
    "gus_tests_avg = round(np.mean(gus_tests),1)\n",
    "\n",
    "print('GUS - Results:')\n",
    "print(f'Best Scenario Training Accuracy: {gus_train_best}%')\n",
    "print(f'Average Training Accuracy: {gus_train_avg}%')\n",
    "print(f'Best Scenario Validation Accuracy: {gus_val_best}%')\n",
    "print(f'Average Validation Accuracy: {gus_vals_avg}%')\n",
    "print(f'Best Scenario Test Accuracy: {gus_test_best}%')\n",
    "print(f'Average Test Accuracy: {gus_tests_avg}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGUS - Results:\n",
      "Best Scenario Training Accuracy: 95.19999999999999%\n",
      "Average Training Accuracy: 95.8%\n",
      "Best Scenario Validation Accuracy: 95.7%\n",
      "Average Validation Accuracy: 97.0%\n",
      "Best Scenario Test Accuracy: 99.1%\n",
      "Average Test Accuracy: 93.4%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Considering just TGUS with other features\n",
    "\n",
    "'''\n",
    "tgus_train =[]\n",
    "tgus_vals =[]\n",
    "tgus_tests=[]\n",
    "for i in range(100):\n",
    "    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_tgus, clf, 'svc')\n",
    "    tgus_train = np.append(train_acc, tgus_train)\n",
    "    tgus_vals = np.append(val_acc, tgus_vals)\n",
    "    tgus_tests = np.append(test_acc, tgus_tests)\n",
    "\n",
    "    # keep best model\n",
    "    if test_acc >= np.max(tgus_tests):\n",
    "        tgus_model = model\n",
    "        tgus_train_best = train_acc\n",
    "        tgus_val_best = val_acc\n",
    "        tgus_test_best = test_acc\n",
    "\n",
    "# get average accuracies\n",
    "tgus_train_avg = round(np.mean(tgus_train),1)\n",
    "tgus_vals_avg = round(np.mean(tgus_vals),1)\n",
    "tgus_tests_avg = round(np.mean(tgus_tests),1)\n",
    "\n",
    "print('TGUS - Results:')\n",
    "print(f'Best Scenario Training Accuracy: {tgus_train_best}%')\n",
    "print(f'Average Training Accuracy: {tgus_train_avg}%')\n",
    "print(f'Best Scenario Validation Accuracy: {tgus_val_best}%')\n",
    "print(f'Average Validation Accuracy: {tgus_vals_avg}%')\n",
    "print(f'Best Scenario Test Accuracy: {tgus_test_best}%')\n",
    "print(f'Average Test Accuracy: {tgus_tests_avg}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nConsidering just TGUS* and raw values with other features\\n\\n\\ntgus_st_train =[]\\ntgus_st_vals =[]\\ntgus_st_tests=[]\\nfor i in range(30):\\n    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_tgus_st, clf, 'svc')\\n    tgus_st_train = np.append(train_acc, tgus_st_train)\\n    tgus_st_vals = np.append(val_acc, tgus_st_vals)\\n    tgus_st_tests = np.append(test_acc, tgus_st_tests)\\n\\n    # keep best model\\n    if test_acc >= np.max(tgus_st_tests):\\n        tgus_st_model = model\\n        tgus_st_train_acc = train_acc\\n        tgus_st_val_acc = val_acc\\n        tgus_st_test_acc = test_acc\\n\\nprint('TGUS* - Results:')\\nprint(f'Best Scenario Training Accuracy: {tgus_st_train_acc}%')\\nprint(f'Average Training Accuracy: {round(np.mean(tgus_st_train),1)}%')\\nprint(f'Best Scenario Validation Accuracy: {tgus_st_val_acc}%')\\nprint(f'Average Validation Accuracy: {round(np.mean(tgus_st_vals),1)}%')\\nprint(f'Best Scenario Test Accuracy: {tgus_st_test_acc}%')\\nprint(f'Average Test Accuracy: {round(np.mean(tgus_st_tests),1)}%')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Considering just TGUS* and raw values with other features\n",
    "\n",
    "\n",
    "tgus_st_train =[]\n",
    "tgus_st_vals =[]\n",
    "tgus_st_tests=[]\n",
    "for i in range(30):\n",
    "    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_tgus_st, clf, 'svc')\n",
    "    tgus_st_train = np.append(train_acc, tgus_st_train)\n",
    "    tgus_st_vals = np.append(val_acc, tgus_st_vals)\n",
    "    tgus_st_tests = np.append(test_acc, tgus_st_tests)\n",
    "\n",
    "    # keep best model\n",
    "    if test_acc >= np.max(tgus_st_tests):\n",
    "        tgus_st_model = model\n",
    "        tgus_st_train_acc = train_acc\n",
    "        tgus_st_val_acc = val_acc\n",
    "        tgus_st_test_acc = test_acc\n",
    "\n",
    "print('TGUS* - Results:')\n",
    "print(f'Best Scenario Training Accuracy: {tgus_st_train_acc}%')\n",
    "print(f'Average Training Accuracy: {round(np.mean(tgus_st_train),1)}%')\n",
    "print(f'Best Scenario Validation Accuracy: {tgus_st_val_acc}%')\n",
    "print(f'Average Validation Accuracy: {round(np.mean(tgus_st_vals),1)}%')\n",
    "print(f'Best Scenario Test Accuracy: {tgus_st_test_acc}%')\n",
    "print(f'Average Test Accuracy: {round(np.mean(tgus_st_tests),1)}%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw - Results:\n",
      "Best Scenario Training Accuracy: 96.3%\n",
      "Average Training Accuracy: 97.2%\n",
      "Best Scenario Validation Accuracy: 95.7%\n",
      "Average Validation Accuracy: 97.3%\n",
      "Best Scenario Test Accuracy: 99.1%\n",
      "Average Test Accuracy: 94.1%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Considering just raw values with other features\n",
    "\n",
    "'''\n",
    "raw_train =[]\n",
    "raw_vals =[]\n",
    "raw_tests=[]\n",
    "for i in range(100):\n",
    "    model, train_acc, val_acc, test_acc = f.kfold_crossval(df_raw, clf, 'bagging')\n",
    "    raw_train = np.append(train_acc, raw_train)\n",
    "    raw_vals = np.append(val_acc, raw_vals)\n",
    "    raw_tests = np.append(test_acc, raw_tests)\n",
    "\n",
    "    # keep best model\n",
    "    if test_acc >= np.max(raw_tests):\n",
    "        raw_model = model\n",
    "        raw_train_best = train_acc\n",
    "        raw_val_best = val_acc\n",
    "        raw_test_best = test_acc\n",
    "\n",
    "\n",
    "# get average accuracies\n",
    "raw_train_avg = round(np.mean(raw_train),1)\n",
    "raw_vals_avg = round(np.mean(raw_vals),1)\n",
    "raw_tests_avg = round(np.mean(raw_tests),1)\n",
    "\n",
    "print('Raw - Results:')\n",
    "print(f'Best Scenario Training Accuracy: {raw_train_best}%')\n",
    "print(f'Average Training Accuracy: {raw_train_avg}%')\n",
    "print(f'Best Scenario Validation Accuracy: {raw_val_best}%')\n",
    "print(f'Average Validation Accuracy: {raw_vals_avg}%')\n",
    "print(f'Best Scenario Test Accuracy: {raw_test_best}%')\n",
    "print(f'Average Test Accuracy: {raw_tests_avg}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ            ‚îÇ  Avg. Train %  ‚îÇ  Avg. Validation %  ‚îÇ  Avg. Test %  ‚îÇ  Best Sc. Train %  ‚îÇ  Best Sc. Validation %  ‚îÇ  Best Sc. Test %  ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ All        ‚îÇ      98.1      ‚îÇ        97.8         ‚îÇ     94.3      ‚îÇ        97.7        ‚îÇ          95.8           ‚îÇ       99.1        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ GUS Focus  ‚îÇ      96.1      ‚îÇ        96.6         ‚îÇ     93.4      ‚îÇ        94.6        ‚îÇ          97.1           ‚îÇ       99.1        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ TGUS Focus ‚îÇ      95.8      ‚îÇ         97          ‚îÇ     93.4      ‚îÇ        95.2        ‚îÇ          95.7           ‚îÇ       99.1        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Raw Focus  ‚îÇ      97.2      ‚îÇ        97.3         ‚îÇ     94.1      ‚îÇ        96.3        ‚îÇ          95.7           ‚îÇ       99.1        ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "# fill results table\n",
    "dfr.loc['All',:] = [all_train_avg, all_vals_avg, all_tests_avg, all_train_best, all_val_best, all_test_best]\n",
    "dfr.loc['GUS Focus',:] = [gus_train_avg, gus_vals_avg, gus_tests_avg, gus_train_best, gus_val_best, gus_test_best]\n",
    "dfr.loc['TGUS Focus',:] = [tgus_train_avg, tgus_vals_avg, tgus_tests_avg, tgus_train_best, tgus_val_best, tgus_test_best]\n",
    "dfr.loc['Raw Focus',:] = [raw_train_avg, raw_vals_avg, raw_tests_avg, raw_train_best, raw_val_best, raw_test_best]\n",
    "\n",
    "print(tabulate(dfr, headers='keys', tablefmt = 'fancy_grid', numalign = 'center'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Equal variance t-tests to compare result means\n",
    "'''\n",
    "cols = ['All', 'GUS', 'TGUS', 'Raw']\n",
    "train_scores = pd.DataFrame({'All': all_train, 'GUS': gus_train, 'TGUS': tgus_train, 'Raw': raw_train})\n",
    "val_scores = pd.DataFrame({'All': all_vals, 'GUS': gus_vals, 'TGUS': tgus_vals, 'Raw': raw_vals})\n",
    "test_scores = pd.DataFrame({'All': all_tests, 'GUS': gus_tests, 'TGUS': tgus_tests, 'Raw': raw_tests})\n",
    "\n",
    "comp_train = pd.DataFrame(columns = train_scores.columns, index = train_scores.columns )\n",
    "comp_val = pd.DataFrame(columns = train_scores.columns, index = train_scores.columns )\n",
    "comp_test = pd.DataFrame(columns = train_scores.columns, index = train_scores.columns )\n",
    "\n",
    "for i in cols:\n",
    "    for j in cols:\n",
    "        stat_train,p_train = sts.ttest_ind(train_scores.loc[:,i], train_scores.loc[:,j], equal_var = True, alternative = 'two-sided')\n",
    "        comp_train.loc[i,j] = [round(p_train,100)]\n",
    "\n",
    "        stat_val,p_val = sts.ttest_ind(val_scores.loc[:,i], val_scores.loc[:,j], equal_var = True, alternative = 'two-sided')\n",
    "        comp_val.loc[i,j] = [round(p_val,100)]\n",
    "\n",
    "        stat_test,p_test = sts.ttest_ind(test_scores.loc[:,i], test_scores.loc[:,j], equal_var = True, alternative = 'two-sided')\n",
    "        comp_test.loc[i,j] = [round(p_test,100)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
