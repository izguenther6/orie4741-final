{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA ORGANIZATION**\n",
    "\n",
    "this script will be used for organizing the data/feature engineering and writing other .csv/xslx files as needed\n",
    "\n",
    "NOTE: the orginal datafile will not be saved in this repository as it contains confidential location information...each location will be assigned a number, and we will keep track of this list internally, however this number will not be used in the algorithms as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and get raw data file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import funcs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#note that 'private_name' is the associated secret number for the different locations\n",
    "df = pd.read_csv('../data/raw_data.csv')\n",
    "t1 = pd.read_excel('../data/table1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT NOTES/ASSUMPTIONS: \n",
    "\n",
    "- many of the tests are for other soil/water parameters (pH, electrical conductivity, etc) so we want to extract just pesticide tests...\n",
    "\n",
    "- to be thorough, the DEC tested for numerous pesticides on each sample, many of which were not applied, resulting in lots of important \n",
    "  but unusable data where there is no detectable amount\n",
    "\n",
    "- many farmers/pesticide appliers provided us information on which pesticides they used...the df includes a 'wasused' column that will be\n",
    "  utilized to extract the usable feature...however many pesticides were detectable in cases where we did not know if it was applied, so it\n",
    "  is ASSUMED that the pesticide was applied somewhere in close proximity\n",
    "\n",
    "- different testing methods with different detection limits are used for different pesticides...these methods/limits are often improving it... \n",
    "  is suspected that the lower the detection limit, the more likely a pesticide is to be detected...so it will be used as a parameter in \n",
    "  the algorithms...some detection limits for the associated 'parameter' were not entered into the dataset for each test, however they were all entered for\n",
    "  at least one test, so we must fill the NaN values correctly\n",
    "  \n",
    "- all tests for sulfur as the parameter will be removed due to wildly varying behavior\n",
    "\n",
    "- uninterested in loctype 'Pond', 'Categorical - potable', and 'Long term' as these were ancillary tests or not enough information \n",
    "  is known about the testing area\n",
    "\n",
    "- FEATURE ENGINEERING: all nan results are considered zero...the pesticide was not detected\n",
    "\n",
    "'''\n",
    "# fill na results to 0\n",
    "df['result'] = df['result'].fillna(0)\n",
    "pd.to_numeric(df['result'])\n",
    "\n",
    "# fill all detection limits\n",
    "df['detlimit'].fillna(0, inplace =True)\n",
    "df['detlimit'] = df['detlimit'].astype(str)\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('*',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('>',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('<',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('?',''))\n",
    "df['detlimit'] = df['detlimit'].astype(float)\n",
    "for idx, row in df.iterrows():\n",
    "    # find unfilled detlimit\n",
    "    if row['detlimit'] == '':\n",
    "        parameter = row['parameter']\n",
    "        detlimit = df[(df['detlimit'] != '') & (df['parameter'] == parameter)]\n",
    "\n",
    "        # fill limit if found elsewhere\n",
    "        if len(detlimit) > 0:\n",
    "            df.loc[idx, 'detlimit'] = detlimit.loc[detlimit.index[0],'detlimit']\n",
    "\n",
    "\n",
    "#this contains all test rows to be put into algorithms\n",
    "df_tests = df[np.logical_or(df['wasused'] != 'no', df['koc'].notnull() & df['result'] > 0, df['kfoc'].notnull() & df['result'] > 0)]\n",
    "df_tests = df_tests[df_tests['drainage_class'].notnull() & df_tests['soil_halflife'].notnull()]\n",
    "df_tests = df_tests[df_tests['parameter'] != 'Sulfur']\n",
    "\n",
    "#strip whitespaces\n",
    "df_tests['loctype'] = df_tests['loctype'].apply(lambda x: x.strip())\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Pond']\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Categorical - potable']\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Long term']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- theoretically, the organic carbon-water partition coefficient ('koc' column) and the organic carbon-water normalized Freundlich distribution \n",
    "  coefficient will be treated as the same\n",
    "\n",
    "- this loop combines the columns, choosing koc first if it is available\n",
    "\n",
    "'''\n",
    "pcoef = []\n",
    "for idx, row in df_tests.iterrows():\n",
    "    if row['koc'] > 0 :\n",
    "        pcoef += [float(row['koc'])]\n",
    "    else :\n",
    "        pcoef += [float(row['kfoc'])]\n",
    "\n",
    "df_tests['pcoef'] = pcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- extract all current columns of potential interest to be put into algorithms...NOT FINAL\n",
    "\n",
    "- other minor fixes\n",
    "'''\n",
    "col_list = ['private_name', 'loctype', 'aquifer_vulnerability', 'drainage_class', 'detlimit', 'sampdate', 'parameter','gus', 'soil_halflife', 'simphalflife', 'morehalflives', 'pcoef', 'simpsorp', 'simpsorp2', 'result', 'simpresult']\n",
    "\n",
    "#get all columns of interest\n",
    "df_cols = df_tests.loc[:, col_list]\n",
    "\n",
    "#replace all instances of 'well drained' to 'Well drained'\n",
    "df_cols.replace(to_replace='well drained', value='Well drained', inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/n7d1lkjj6y1_lcp76m8nrtbh0000gn/T/ipykernel_91900/2876063235.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_adjusted.loc[idx, 'detected'] = -1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT:\n",
    "\n",
    "- at many testing sites, samples were taken in both the downgradient and upgradient groundwater of the pesticide-treated area...\n",
    "  these are distinguished by 'Categorical - upgradient' and'Categorical - downgradiet'...'Categorical - up and downgradient' indicates\n",
    "  one site where the test was both upgradient of one treated area and downgradient of another\n",
    "\n",
    "- tests were done at upgradient sites to find out if pesticides were in the already in the groundwater NOT as a result of the land-owners'\n",
    "  application...this could be the result of a neighboring property apply pesticides, for example...if the same pesticide is detected downgradient \n",
    "  and upgradient of the pesticide application area, then the upgradient value should be subtracted from the downgradient value to get a better\n",
    "  representation of what is happening with land-owners' pesticides\n",
    "\n",
    "- this loop identifies upgradient/downgradient tests on the same sampling date and subtracts the upgradient result from the downgradient\n",
    "\n",
    "'''\n",
    "\n",
    "# reset index\n",
    "df_reset = df_cols.reset_index().iloc[:,1:]\n",
    "\n",
    "for idx, row in df_reset.iterrows():\n",
    "    # find 'upgradient' or 'up and downgradient' test on same date for same parameter in same location\n",
    "    if row['loctype'] in ['Categorical - downgradient','Categorical - up and downgradient']:\n",
    "        sampdate = row['sampdate']\n",
    "        parameter = row['parameter']\n",
    "        loctype = row['loctype']\n",
    "        name = row['private_name']\n",
    "        upgradient = df_reset[(df_reset['private_name'] == name) & (df_reset['sampdate'] == sampdate) & (df_reset['loctype'] > loctype) & (df_reset['parameter'] == parameter)]\n",
    "\n",
    "    # if test has both 'upgradient' and 'up and downgradient' samples, then subtract just the 'up and downgradient'\n",
    "    # when upgradient is created, it puts 'up and downgradient' results first, so we can just subtract out first index of whatever upgradient is\n",
    "    if len(upgradient) > 0:\n",
    "      df_reset.loc[idx, 'result'] -= upgradient.loc[upgradient.index[0],'result']\n",
    "\n",
    "# now extract out just the downgradient tests of interests\n",
    "df_adjusted = df_reset[(df_reset['loctype'] != 'Categorical - upgradient') & (df_reset['loctype'] != 'Categorical - up and downgradient') ]\n",
    "\n",
    "\n",
    "# add a 'detected' column if result > 0\n",
    "# 1 if detected, -1 if not\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "    if df_adjusted.loc[idx, 'result'] > 0:\n",
    "        df_adjusted.loc[idx, 'detected'] = 1\n",
    "    else:\n",
    "        df_adjusted.loc[idx, 'detected'] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index again\n",
    "df_adjusted = df_adjusted.reset_index().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aldicarb' 'Atrazine' 'Diuron' ' Metolachlor' 'Oxamyl' 'Picloram'\n",
      " 'Prometryn' 'Simazine' 'Chlordane' 'Chlorothalonil' 'Chlorpyrifos'\n",
      " '2,4-D' 'Dicamba' 'Endosulfan' 'Heptachlor' 'Lindane' 'Phorate'\n",
      " 'Propachlor' 'Toxaphene' 'Trifluralin' 'Alachlor' 'Carbaryl' 'Carbofuran'\n",
      " 'Dinoseb' 'Ethoprop' 'Fonofos' 'Chlorthal dimethyl' 'Cyanazine' 'DBCP'\n",
      " 'EDB ' 'Metribuzin' 'Naled' 'Prometon' 'Propylene dichloride' 'Aldrin'\n",
      " 'Chloramben' '1,3-D' 'DDD' 'Endosulfan sulfate' 'Pendimethalin' 'Silvex'\n",
      " 'DDT' 'Endrin' 'Dieldrin']\n",
      "['Boscalid' 'Myclobutanil' 'Chlorpyrifos' 'Oxadiazon' 'Metolachlor OA'\n",
      " 'Metolachlor ESA' 'Imidacloprid' 'Indaziflam' 'Atrazine' 'Linuron'\n",
      " 'S-Metolachlor' 'Carbaryl' 'Flumioxazin' 'Glyphosate' 'Ethofumesate'\n",
      " 'Acetamiprid' 'Malathion' 'Metribuzin' 'Fluopyram' 'Sulfentrazone'\n",
      " 'Terbacil' 'Diuron' 'Acetochlor ESA' 'Simazine' 'Tebuconazole'\n",
      " 'Thiamethoxam' 'Mandipropamid' 'Clethodim' 'JSE76' 'Mefentrifluconazole'\n",
      " 'Pyrimethanil' 'Iprodione' 'Propiconazole' 'Bentazon'\n",
      " 'Chlorantraniliprole' 'Fluazinam' 'Fluxapyroxad' 'Paclobutrazol'\n",
      " 'Clopyralid' 'Dithiopyr' 'Halosulfuron-methyl' 'Triadimefon'\n",
      " 'De Ethyl Atrazine' '2,4-D' 'Dicamba']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(t1['parameter'].unique())\n",
    "print(df_adjusted['parameter'].unique())\n",
    "print('2,4-D' in t1['parameter'].un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- SWL lab members are currently deriving a new theoretical groundwater ubiquity score (TGUS) to be compared to typically used groundwater ubiquity\n",
    "  score (GUS) derived by Gustafson et al., 1989\n",
    "\n",
    "- dataframe 't1' contains columns for the GUS, TGUS, and TGUS* (a modified form of TGUS) for 45 different pesticides, as well as some more accurate\n",
    "  soil halflife and partitioning coefficient values that need to be updated in our data\n",
    "\n",
    "- we will consider the effect of all ubiquity scores together and separately for predicting test outcomes\n",
    "\n",
    "- many tgus and tgus* values are not documented, so those need to be calculated using defined functions\n",
    "\n",
    "'''\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "  parameter = row['parameter']\n",
    "\n",
    "  if parameter in t1['parameter'].unique():\n",
    "    # get needed values from t1\n",
    "    pcoef = t1[t1['parameter'] == parameter]['koc']\n",
    "    shl = t1[t1['parameter'] == parameter]['soil_halflife']\n",
    "    gus = t1[t1['parameter'] == parameter]['gus']\n",
    "    tgus = t1[t1['parameter'] == parameter]['tgus']\n",
    "    tgus_star = t1[t1['parameter'] == parameter]['tgus*']\n",
    "\n",
    "    # add values to data\n",
    "    df_adjusted.loc[idx, 'pcoef'] = pcoef.iloc[0]\n",
    "    df_adjusted.loc[idx, 'soil_halflife'] = shl.iloc[0]\n",
    "    df_adjusted.loc[idx, 'gus'] = gus.iloc[0]\n",
    "    df_adjusted.loc[idx, 'tgus'] = tgus.iloc[0]\n",
    "    df_adjusted.loc[idx, 'tgus*'] = tgus_star.iloc[0]\n",
    "\n",
    "  else:\n",
    "    tgus_star = funcs.tgus(row['soil_halflife'], row['pcoef'], star = True)\n",
    "    tgus = funcs.tgus(row['soil_halflife'], row['pcoef'])\n",
    "    df_adjusted.loc[idx, 'tgus*'] = tgus_star\n",
    "    df_adjusted.loc[idx, 'tgus'] = tgus\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup final dataframe\n",
    "# for now, working with all raw numbers and not pre-decided categories\n",
    "onehot_cols = ['aquifer_vulnerability','drainage_class']\n",
    "raw_cols = ['gus','tgus', 'tgus*','soil_halflife', 'pcoef']\n",
    "\n",
    "# normalize raw values\n",
    "norm = scaler.fit_transform(df_adjusted.loc[:, raw_cols])\n",
    "norm = round(pd.DataFrame(norm, columns = raw_cols), 3)\n",
    "\n",
    "# onehot categorical\n",
    "df_onehot = funcs.onehot(df=df_adjusted, columns = onehot_cols)\n",
    "df_final = pd.concat([df_onehot, norm], axis = 1)\n",
    "\n",
    "# append offset and re-add detected column\n",
    "df_final['offset'] = np.ones((df_adjusted.shape[0]))\n",
    "df_final['detected'] = df_adjusted['detected']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- to compare the performance of the different groundwater ubiquity score, values, we will make separate dataframes containing just one of the score values\n",
    "\n",
    "- we will also separate out a dataframe with just the soil halflives/partitioning coefficient and no ubiquity scores to see how well raw values perform\n",
    "\n",
    "- 'df_final' will be used to evaluate the performance of all ubiquity scores and raw data combined\n",
    "\n",
    "'''\n",
    "\n",
    "df_gus = df_final.loc[:, ~df_final.columns.isin(['tgus', 'tgus*', 'soil_halflife', 'pcoef'])]\n",
    "df_tgus = df_final.loc[:, ~df_final.columns.isin(['gus', 'tgus*', 'soil_halflife', 'pcoef'])]\n",
    "df_tgus_st = df_final.loc[:, ~df_final.columns.isin(['tgus', 'gus', 'soil_halflife', 'pcoef'])]\n",
    "df_raw = df_final.loc[:, ~df_final.columns.isin(['tgus', 'tgus*', 'gus'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write df_final as csv for future use\n",
    "df_final.to_csv(path_or_buf = '../data/df_all.csv', sep = ',')\n",
    "df_gus.to_csv(path_or_buf = '../data/df_gus.csv', sep = ',')\n",
    "df_tgus.to_csv(path_or_buf = '../data/df_tgus.csv', sep = ',')\n",
    "df_tgus_st.to_csv(path_or_buf = '../data/df_tgus*.csv', sep = ',')\n",
    "df_raw.to_csv(path_or_buf = '../data/df_raw.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
