{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA ORGANIZATION** ðŸ§º\n",
    "\n",
    "this script will be used for organizing the data/feature engineering and writing other .csv/xslx files as needed\n",
    "\n",
    "NOTE: the orginal datafile will not be saved in this repository as it contains confidential location information...each location will be assigned a number, and we will keep track of this list internally, however this number will not be used in the algorithms as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and get raw data file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import funcs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# note that 'private_name' is the associated secret number for the different locations\n",
    "# df is importated from local machine to protect privacy\n",
    "df = pd.read_csv('/Users/isaiah/Desktop/swl/nys pesticides/data/raw_data.csv')\n",
    "df2 = pd.read_csv('/Users/isaiah/Desktop/swl/nys pesticides/data/raw_data.csv')\n",
    "soil_params = pd.read_excel('/Users/isaiah/Desktop/swl/nys pesticides/data/Soil and Pestecide paramaters.xlsx', \n",
    "                            sheet_name='soil parameters')\n",
    "pest_params = pd.read_excel('/Users/isaiah/Desktop/swl/nys pesticides/data/Soil and Pestecide paramaters.xlsx', \n",
    "                            sheet_name='Pestecide parameters')\n",
    "t1 = pd.read_excel('../data/table1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT NOTES/ASSUMPTIONS: \n",
    "\n",
    "- many of the tests are for other soil/water parameters (pH, electrical conductivity, etc) so we want to extract just pesticide tests...\n",
    "\n",
    "- to be thorough, the DEC tested for numerous pesticides on each sample, many of which were not applied, resulting in lots of important \n",
    "  but unusable data where there is no detectable amount\n",
    "\n",
    "- many farmers/pesticide appliers provided us information on which pesticides they used...the df includes a 'wasused' column that will be\n",
    "  utilized to extract the usable feature...however many pesticides were detectable in cases where we did not know if it was applied, so it\n",
    "  is ASSUMED that the pesticide was applied somewhere in close proximity\n",
    "\n",
    "- different testing methods with different detection limits are used for different pesticides...these methods/limits are often improving it... \n",
    "  is suspected that the lower the detection limit, the more likely a pesticide is to be detected...so it will be used as a parameter in \n",
    "  the algorithms...some detection limits for the associated 'parameter' were not entered into the dataset for each test, however they were all entered for\n",
    "  at least one test, so we must fill the NaN values correctly\n",
    "  \n",
    "- all tests for sulfur as the parameter will be removed due to wildly varying behavior\n",
    "\n",
    "- uninterested in loctype 'Pond', 'Categorical - potable', and 'Long term' as these were ancillary tests or not enough information \n",
    "  is known about the testing area\n",
    "\n",
    "- FEATURE ENGINEERING: all nan results are considered zero...the pesticide was not detected\n",
    "\n",
    "'''\n",
    "# fill na results to 0\n",
    "df['result'] = df['result'].fillna(0)\n",
    "pd.to_numeric(df['result'])\n",
    "\n",
    "# fill all detection limits\n",
    "df['detlimit'] = df['detlimit'].astype(str)\n",
    "for idx, row in df.iterrows():\n",
    "    if row['detlimit'] == 'nan':\n",
    "        parameter = row['parameter']\n",
    "        detlimit = df[(df['detlimit'] != 'nan') & (df['parameter'] == parameter)]\n",
    "\n",
    "        # fill limit if found elsewhere\n",
    "        if len(detlimit) > 0:\n",
    "            year = row['sampdate'][0:4]\n",
    "            for idx2, row2 in detlimit.iterrows():\n",
    "                if row2['sampdate'][0:4] == year:\n",
    "                    df.loc[idx, 'detlimit'] = detlimit.loc[idx2,'detlimit']\n",
    "                    break\n",
    "\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('*',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('>',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('<',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('?',''))\n",
    "df['detlimit'] = df['detlimit'].astype(float)\n",
    "\n",
    "#this contains all test rows to be put into algorithms\n",
    "df_tests = df[np.logical_or((df['wasused'] != 'no') &  (df['wasused'].notnull()), df['koc'].notnull() & df['result'] > 0, df['kfoc'].notnull() & df['result'] > 0)]\n",
    "df_tests = df_tests[df_tests['drainage_class'].notnull() & df_tests['soil_halflife'].notnull()]\n",
    "df_tests = df_tests[df_tests['parameter'] != 'Sulfur']\n",
    "\n",
    "#strip whitespaces, make some string columns lowercase, get rid of some rows\n",
    "df_tests['loctype'] = df_tests['loctype'].apply(lambda x: x.strip())\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Pond']\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Categorical - potable']\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Long term']\n",
    "df_tests['site_code'] = df_tests['site_code'].str.lower()\n",
    "df_tests['loccode'] = df_tests['loccode'].str.lower()\n",
    "soil_params['Identify'] = soil_params['Identify'].str.lower()\n",
    "soil_params['loccode'] = soil_params['loccode'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "- we are now working with a more developed TGUS equation with more soil/pesticide characteristics\n",
    "\n",
    "- not all pesticides included in the usable list thusfar have this information documented...need to remove these\n",
    "\n",
    "- also need to add the partition coefficient, pesticide applied amount, the organic matter percent, and the soil bulk densitiy from pest_params and soil_params\n",
    "'''\n",
    "params_not_in = [] #documents which parameters from usable results are not in pest_params, for personal use\n",
    "scode_not_in = []\n",
    "lcode_not_in = []\n",
    "kocs = [] #partitioning coefficients\n",
    "apps = [] #pesticide applied amount\n",
    "oms = [] #percent soil organic matter\n",
    "bulks = [] #soil bulk densities\n",
    "sand = [] #soil percent sand\n",
    "silt = [] #soil percent silt\n",
    "clay = [] #soil percent clay\n",
    "\n",
    "# first get rid of non-included pesticides and sites\n",
    "for idx, row in df_tests.iterrows():\n",
    "    param = row['parameter']\n",
    "    scode = row['site_code']\n",
    "    lcode = row['loccode']\n",
    "    if (param not in np.array(pest_params.loc[:,'parameter']) and param not in params_not_in):\n",
    "        params_not_in.append(param)\n",
    "        df_tests = df_tests[df_tests['parameter'] != param]\n",
    "    elif (scode not in np.array(soil_params.loc[:,'Identify']) and scode not in scode_not_in):\n",
    "        scode_not_in.append(scode)\n",
    "        df_tests = df_tests[df_tests['site_code'] != scode]\n",
    "    elif (lcode not in np.array(soil_params.loc[:,'loccode']) and lcode not in lcode_not_in):\n",
    "        lcode_not_in.append(lcode)\n",
    "        df_tests = df_tests[(df_tests['site_code'] != scode) & (df_tests['loccode'] != lcode)]\n",
    "\n",
    "# loop thru again to get necessary data\n",
    "for idx, row in df_tests.iterrows():\n",
    "    param = row['parameter']\n",
    "    scode = row['site_code']\n",
    "    lcode = row['loccode']\n",
    "    kocs += [float(pest_params[pest_params['parameter'] == param].loc[:,'KOC (m^3/Mg)=(cm^3/gr)'])]\n",
    "    apps += [float(pest_params[pest_params['parameter'] == param].loc[:,'apllayd amoint (ug/Hectar)'])]\n",
    "    oms += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Organic matter (%)'])]\n",
    "    bulks += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Bulk density (gr/cm3)'])]\n",
    "    sand += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Sand %'])]\n",
    "    silt += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Silt %'])]\n",
    "    clay += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Clay %'])]\n",
    "\n",
    "\n",
    "# append necessary data\n",
    "df_tests.loc[:,['koc [m^3/Mg]','application rates [mg/m^2]', 'organic matter [%]', 'bulk density [Mg/m^3]',\n",
    "                 'Sand %', 'Silt %', 'Clay %']] = np.array([kocs, apps, oms, bulks, sand, silt, clay]).T\n",
    "\n",
    "# convert necessary units for TGUS equation\n",
    "df_tests['application rates [mg/m^2]'] = df_tests['application rates [mg/m^2]'] * 0.0000001\n",
    "#df_tests = df_tests[df_tests['bulk density [Mg/m^3]'] >= 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% = inorganic + organic, in = (1 - om) \n",
    ".67 = in_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6380952380952382"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    }
   ],
   "source": [
    "print(len(df_tests[df_tests['bulk density [Mg/m^3]'] >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNO LONGER USING !!!!!!!!!\\n\\nIMPORTANT CONCEPT\\n\\n- theoretically, the organic carbon-water partition coefficient ('koc' column) and the organic carbon-water normalized Freundlich distribution \\n  coefficient will be treated as the same\\n\\n- this loop combines the columns, choosing koc first if it is available\\n\\n\\npcoef = []\\nfor idx, row in df_tests.iterrows():\\n    if row['koc'] > 0 :\\n        pcoef += [float(row['koc'])]\\n    else :\\n        pcoef += [float(row['kfoc'])]\\n\\ndf_tests['pcoef'] = pcoef\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NO LONGER USING !!!!!!!!!\n",
    "\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- theoretically, the organic carbon-water partition coefficient ('koc' column) and the organic carbon-water normalized Freundlich distribution \n",
    "  coefficient will be treated as the same\n",
    "\n",
    "- this loop combines the columns, choosing koc first if it is available\n",
    "\n",
    "\n",
    "pcoef = []\n",
    "for idx, row in df_tests.iterrows():\n",
    "    if row['koc'] > 0 :\n",
    "        pcoef += [float(row['koc'])]\n",
    "    else :\n",
    "        pcoef += [float(row['kfoc'])]\n",
    "\n",
    "df_tests['pcoef'] = pcoef\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- extract all current columns of potential interest to be put into algorithms...NOT FINAl!!!!!!!!!!!!!!\n",
    "\n",
    "- other minor fixes\n",
    "'''\n",
    "col_list = ['private_name', 'loctype', 'aquifer_vulnerability', 'drainage_class', 'detlimit', 'sampdate', 'parameter','gus', \n",
    "            'soil_halflife', 'simphalflife', 'morehalflives', 'koc [m^3/Mg]','application rates [mg/m^2]', 'organic matter [%]', \n",
    "            'bulk density [Mg/m^3]', 'Sand %', 'Silt %', 'Clay %', 'simpsorp', 'simpsorp2', 'result', 'simpresult']\n",
    "\n",
    "#get all columns of interest\n",
    "df_cols = df_tests.loc[:, col_list]\n",
    "\n",
    "#replace all instances of 'well drained' to 'Well drained'\n",
    "df_cols.replace(to_replace='well drained', value='Well drained', inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT:\n",
    "\n",
    "- at many testing sites, samples were taken in both the downgradient and upgradient groundwater of the pesticide-treated area...\n",
    "  these are distinguished by 'Categorical - upgradient' and'Categorical - downgradient'...'Categorical - up and downgradient' indicates\n",
    "  one site where the test was both upgradient of one treated area and downgradient of another\n",
    "\n",
    "- tests were done at upgradient sites to find out if pesticides were in the already in the groundwater NOT as a result of the land-owners'\n",
    "  application...this could be the result of a neighboring property apply pesticides, for example...if the same pesticide is detected downgradient \n",
    "  and upgradient of the pesticide application area, then the upgradient value should be subtracted from the downgradient value to get a better\n",
    "  representation of what is happening with land-owners' pesticides\n",
    "\n",
    "- this loop identifies upgradient/downgradient tests on the same sampling date and subtracts the upgradient result from the downgradient\n",
    "\n",
    "'''\n",
    "\n",
    "# reset index\n",
    "df_reset = df_cols.reset_index().iloc[:,1:]\n",
    "\n",
    "'''\n",
    "THIS IS NOT BEING DONE NOW!\n",
    "\n",
    "for idx, row in df_reset.iterrows():\n",
    "    # find 'upgradient' or 'up and downgradient' test on same date for same parameter in same location\n",
    "    if row['loctype'] in ['Categorical - downgradient','Categorical - up and downgradient']:\n",
    "        sampdate = row['sampdate']\n",
    "        parameter = row['parameter']\n",
    "        loctype = row['loctype']\n",
    "        name = row['private_name']\n",
    "        upgradient = df_reset[(df_reset['private_name'] == name) & (df_reset['sampdate'] == sampdate) & (df_reset['loctype'] > loctype) & (df_reset['parameter'] == parameter)]\n",
    "\n",
    "    # if test has both 'upgradient' and 'up and downgradient' samples, then subtract just the 'up and downgradient'\n",
    "    # when upgradient is created, it puts 'up and downgradient' results first, so we can just subtract out first index of whatever upgradient is\n",
    "    if len(upgradient) > 0:\n",
    "      df_reset.loc[idx, 'result'] -= upgradient.loc[upgradient.index[0],'result']\n",
    "\n",
    "# now extract out just the downgradient tests of interests\n",
    "# UNDECIDED ON THIS! for now just copy df_reset\n",
    "#df_adjusted = df_reset[(df_reset['loctype'] != 'Categorical - upgradient') & (df_reset['loctype'] != 'Categorical - up and downgradient') ]\n",
    "'''\n",
    "df_adjusted = df_reset.loc[:,:]\n",
    "\n",
    "\n",
    "# add a 'detected' column if result > 0\n",
    "# 1 if detected, -1 if not\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "    if df_adjusted.loc[idx, 'result'] > 0:\n",
    "        df_adjusted.loc[idx, 'detected'] = 1\n",
    "    else:\n",
    "        df_adjusted.loc[idx, 'detected'] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index again\n",
    "df_adjusted = df_adjusted.reset_index().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n  if parameter in t1['parameter'].unique():\\n    # get needed values from t1\\n    pcoef = t1[t1['parameter'] == parameter]['koc']\\n    shl = t1[t1['parameter'] == parameter]['soil_halflife']\\n    gus = t1[t1['parameter'] == parameter]['gus']\\n    tgus = t1[t1['parameter'] == parameter]['tgus']\\n    tgus_star = t1[t1['parameter'] == parameter]['tgus*']\\n\\n    # add values to data\\n    df_adjusted.loc[idx, 'pcoef'] = pcoef.iloc[0]\\n    df_adjusted.loc[idx, 'soil_halflife'] = shl.iloc[0]\\n    df_adjusted.loc[idx, 'gus'] = gus.iloc[0]\\n    df_adjusted.loc[idx, 'tgus'] = tgus.iloc[0]\\n    df_adjusted.loc[idx, 'tgus*'] = tgus_star.iloc[0]\\n\\n  else:\\n    tgus_star = funcs.tgus(row['soil_halflife'], row['pcoef'], star = True)\\n    tgus = funcs.tgus(row['soil_halflife'], row['pcoef'])\\n    df_adjusted.loc[idx, 'tgus*'] = tgus_star\\n    df_adjusted.loc[idx, 'tgus'] = tgus\\n\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- SWL lab members are currently deriving a new theoretical groundwater ubiquity score (TGUS) to be compared to typically used groundwater ubiquity\n",
    "  score (GUS) derived by Gustafson et al., 1989\n",
    "\n",
    "- dataframe 't1' contains columns for the GUS, TGUS, and TGUS* (a modified form of TGUS) for 45 different pesticides, as well as some more accurate\n",
    "  soil halflife and partitioning coefficient values that need to be updated in our data\n",
    "\n",
    "- we will consider the effect of all ubiquity scores together and separately for predicting test outcomes\n",
    "\n",
    "- many tgus and tgus* values are not documented, so those need to be calculated using defined functions\n",
    "\n",
    "'''\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "  tgus = funcs.tgus(row['soil_halflife'], row['application rates [mg/m^2]'], row['detlimit'], row['organic matter [%]'],\n",
    "                    row['bulk density [Mg/m^3]'], row['koc [m^3/Mg]'])\n",
    "  df_adjusted.loc[idx, 'tgus'] = tgus\n",
    "'''\n",
    "\n",
    "  if parameter in t1['parameter'].unique():\n",
    "    # get needed values from t1\n",
    "    pcoef = t1[t1['parameter'] == parameter]['koc']\n",
    "    shl = t1[t1['parameter'] == parameter]['soil_halflife']\n",
    "    gus = t1[t1['parameter'] == parameter]['gus']\n",
    "    tgus = t1[t1['parameter'] == parameter]['tgus']\n",
    "    tgus_star = t1[t1['parameter'] == parameter]['tgus*']\n",
    "\n",
    "    # add values to data\n",
    "    df_adjusted.loc[idx, 'pcoef'] = pcoef.iloc[0]\n",
    "    df_adjusted.loc[idx, 'soil_halflife'] = shl.iloc[0]\n",
    "    df_adjusted.loc[idx, 'gus'] = gus.iloc[0]\n",
    "    df_adjusted.loc[idx, 'tgus'] = tgus.iloc[0]\n",
    "    df_adjusted.loc[idx, 'tgus*'] = tgus_star.iloc[0]\n",
    "\n",
    "  else:\n",
    "    tgus_star = funcs.tgus(row['soil_halflife'], row['pcoef'], star = True)\n",
    "    tgus = funcs.tgus(row['soil_halflife'], row['pcoef'])\n",
    "    df_adjusted.loc[idx, 'tgus*'] = tgus_star\n",
    "    df_adjusted.loc[idx, 'tgus'] = tgus\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup final dataframe\n",
    "# for now, working with all raw numbers and not pre-decided categories\n",
    "cat_cols = ['aquifer_vulnerability','drainage_class']\n",
    "raw_cols = ['gus','tgus','soil_halflife', 'koc [m^3/Mg]','detlimit', 'Sand %', 'Silt %', 'Clay %']\n",
    "\n",
    "# normalize raw values ONLY IF NOT DOING TREES\n",
    "#norm = scaler.fit_transform(df_adjusted.loc[:, raw_cols])\n",
    "#norm = round(pd.DataFrame(norm, columns = raw_cols), 3)\n",
    "\n",
    "# ordinal encoder for categoricals\n",
    "df_cats= funcs.ordinal(df_adjusted, 'drainage_class')\n",
    "df_cats = funcs.ordinal(df_cats, 'aquifer_vulnerability')\n",
    "cats = df_cats.loc[:, cat_cols]\n",
    "'''\n",
    "# onehot categorical\n",
    "df_onehot = funcs.onehot(df=df_adjusted, columns = onehot_cols)\n",
    "df_final = pd.concat([df_onehot, norm], axis = 1)\n",
    "'''\n",
    "\n",
    "# combine, then append offset and re-add detected column\n",
    "df_final = pd.concat([cats, df_adjusted.loc[:, raw_cols]], axis = 1)\n",
    "df_final['offset'] = np.ones((df_adjusted.shape[0]))\n",
    "df_final['detected'] = df_adjusted['detected']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- to compare the performance of the different groundwater ubiquity score, values, we will make separate dataframes containing just one of the score values\n",
    "\n",
    "- we will also separate out a dataframe with just the soil halflives/partitioning coefficient and no ubiquity scores to see how well raw values perform\n",
    "\n",
    "- 'df_final' will be used to evaluate the performance of all ubiquity scores and raw data combined\n",
    "\n",
    "'''\n",
    "df_a = df_final.loc[:, ~df_final.columns.isin(['soil_halflife', 'koc [m^3/Mg]', 'Sand %', 'Silt %', 'Clay %', 'aquifer_vulnerability'])]#'soil_halflife', 'koc [m^3/Mg]','detlimit'])]\n",
    "df_b = df_final.loc[:, ~df_final.columns.isin(['tgus','soil_halflife', 'koc [m^3/Mg]', 'Sand %', 'Silt %', 'Clay %', 'aquifer_vulnerability'])]\n",
    "df_c = df_final.loc[:, ~df_final.columns.isin(['gus','soil_halflife', 'koc [m^3/Mg]',  'Sand %', 'Clay %', 'Silt %', 'aquifer_vulnerability'])]#,'aquifer_vulnerability'])]\n",
    "#df_raw = df_final.loc[:, ~df_final.columns.isin(['tgus', 'tgus*', 'gus','detlimit'])]\n",
    "df_d = df_final.loc[:, ~df_final.columns.isin(['soil_halflife', 'koc [m^3/Mg]','aquifer_vulnerability', 'detlimit', 'Sand %', 'Silt %', 'Clay %', 'drainage_class'])]#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write df_final as csv for future use\n",
    "df_a.to_csv(path_or_buf = '../data/df_a.csv', sep = ',')\n",
    "df_b.to_csv(path_or_buf = '../data/df_b.csv', sep = ',')\n",
    "df_c.to_csv(path_or_buf = '../data/df_c.csv', sep = ',')\n",
    "#df_raw.to_csv(path_or_buf = '../data/df_raw.csv', sep = ',')\n",
    "df_d.to_csv(path_or_buf = '../data/df_d.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
