{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA ORGANIZATION** ðŸ§º\n",
    "\n",
    "this script will be used for organizing the data/feature engineering and writing other .csv/xslx files as needed\n",
    "\n",
    "NOTE: the orginal datafile will not be saved in this repository as it contains confidential location information...each location will be assigned a number, and we will keep track of this list internally, however this number will not be used in the algorithms as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and get raw data file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import funcs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# note that 'private_name' is the associated secret number for the different locations\n",
    "# df is importated from local machine to protect privacy\n",
    "df = pd.read_csv('/Users/isaiah/Desktop/swl/nys pesticides/data/raw_data.csv')\n",
    "df2 = pd.read_csv('/Users/isaiah/Desktop/swl/nys pesticides/data/raw_data.csv')\n",
    "soil_params = pd.read_excel('/Users/isaiah/Desktop/swl/nys pesticides/data/Soil and Pestecide paramaters.xlsx', \n",
    "                            sheet_name='soil parameters')\n",
    "pest_params = pd.read_excel('/Users/isaiah/Desktop/swl/nys pesticides/data/Soil and Pestecide paramaters.xlsx', \n",
    "                            sheet_name='Pestecide parameters')\n",
    "t1 = pd.read_excel('../data/table1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTERING PROCESS SUMMARY:\n",
    "\n",
    "1. { Fill NaN results as 0 } - A NaN result indicates a nondetect in the case of pesticides.\n",
    "\n",
    "2. { Fill NaN detection limits } - Many detection limits were not included in the original datafile if there was no detection. However, all\n",
    "detection limits for scenario were included at least once somewhere else in the dataset. The values are found by searching for the same\n",
    "pesticide and sample year.\n",
    "\n",
    "3. { Find the actual pesticide tests } - Many of the tests are for other soil/water parameters (pH, electrical conductivity, etc) \n",
    "so we want to extract just pesticide tests. The dataset is filtered by a logical OR statement of 3 separate AND statements:\n",
    "  - Column 'wasused' does not equal 'no'  AND  column 'wasused' is not NaN || identifies pesticides that were used or maybe used at the site\n",
    "  or a neighboring site\n",
    "\n",
    "OR\n",
    "\n",
    "  - Column 'koc' is not NaN AND column 'result' is > 0 || identifies detections despite 'wasused' being 'no' or NaN\n",
    "\n",
    "OR\n",
    "\n",
    "- Column 'kfoc' is not NaN AND column 'result' is > 0 || same idea as previous statement, but in case the 'koc' value is not listed\n",
    "\n",
    "4. { Filter out NaN drainage classes and soil halflives}  - Some drainage classes and soil halflives were not included in the dataset.\n",
    "\n",
    "5. { Filter out Sulfur } - Sulfur had some odd results and was removed until furthur notice.\n",
    "\n",
    "6. { Filter out certain location types } - We are uninterested in loctype 'Pond', 'Categorical - potable', and 'Long term' as these were ancillary tests or not enough information \n",
    "  is known about the testing area\n",
    "\n",
    "'''\n",
    "\n",
    "# 1. fill na results to 0\n",
    "df['result'] = df['result'].fillna(0)\n",
    "pd.to_numeric(df['result'])\n",
    "\n",
    "# 2. fill all detection limits\n",
    "df['detlimit'] = df['detlimit'].astype(str)\n",
    "for idx, row in df.iterrows():\n",
    "    if row['detlimit'] == 'nan':\n",
    "        parameter = row['parameter']\n",
    "        detlimit = df[(df['detlimit'] != 'nan') & (df['parameter'] == parameter)]\n",
    "\n",
    "        # fill limit if found elsewhere\n",
    "        if len(detlimit) > 0:\n",
    "            year = row['sampdate'][0:4]\n",
    "            for idx2, row2 in detlimit.iterrows():\n",
    "                if row2['sampdate'][0:4] == year:\n",
    "                    df.loc[idx, 'detlimit'] = detlimit.loc[idx2,'detlimit']\n",
    "                    break\n",
    "\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('*',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('>',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('<',''))\n",
    "df['detlimit'] = df['detlimit'].apply(lambda x: x.replace('?',''))\n",
    "df['detlimit'] = df['detlimit'].astype(float)\n",
    "\n",
    "# 3/4/5. find actual tests, filter drainage class and soil halflives, and remove sulfur\n",
    "df_tests = df[np.logical_or((df['wasused'] != 'no') &  (df['wasused'].notnull()), df['koc'].notnull() & df['result'] > 0, df['kfoc'].notnull() & df['result'] > 0)]\n",
    "df_tests = df_tests[df_tests['drainage_class'].notnull() & df_tests['soil_halflife'].notnull()]\n",
    "df_tests = df_tests[df_tests['parameter'] != 'Sulfur']\n",
    "\n",
    "# 6. remove certain loctypes\n",
    "df_tests['loctype'] = df_tests['loctype'].apply(lambda x: x.strip())\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Pond']\n",
    "df_tests = df_tests[df_tests['loctype'] != 'Categorical - potable']\n",
    "#df_tests = df_tests[df_tests['loctype'] != 'Long term']\n",
    "df_tests['site_code'] = df_tests['site_code'].str.lower()\n",
    "df_tests['loccode'] = df_tests['loccode'].str.lower()\n",
    "soil_params['Identify'] = soil_params['Identify'].str.lower()\n",
    "soil_params['loccode'] = soil_params['loccode'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTERING PROCESS CONTINUED\n",
    "\n",
    "7. { Find usable tests for TGUS equation } - Our current documentation is on pesticide/site/location parameters for the TGUS equation \n",
    "is not completed, so the nonusable tests are filtered out. Documentation is found in the df's 'pest_params' and 'soil_params'.\n",
    "\n",
    "'''\n",
    "# 7. filter by usable TGUS tests\n",
    "#documents which parameters/site codes/location codes from usable results are not in pest_params, for personal use\n",
    "params_not_in = [] \n",
    "scode_not_in = []\n",
    "lcode_not_in = []\n",
    "\n",
    "kocs = [] #partitioning coefficients\n",
    "apps = [] #pesticide applied amount\n",
    "oms = [] #percent soil organic matter\n",
    "bulks = [] #soil bulk densities\n",
    "sand = [] #soil percent sand\n",
    "silt = [] #soil percent silt\n",
    "clay = [] #soil percent clay\n",
    "\n",
    "# first get rid of non-included pesticides and sites\n",
    "for idx, row in df_tests.iterrows():\n",
    "    param = row['parameter']\n",
    "    scode = row['site_code']\n",
    "    lcode = row['loccode']\n",
    "    if (param not in np.array(pest_params.loc[:,'parameter']) and param not in params_not_in):\n",
    "        params_not_in.append(param)\n",
    "        df_tests = df_tests[df_tests['parameter'] != param]\n",
    "    elif (scode not in np.array(soil_params.loc[:,'Identify']) and scode not in scode_not_in):\n",
    "        scode_not_in.append(scode)\n",
    "        df_tests = df_tests[df_tests['site_code'] != scode]\n",
    "    elif (lcode not in np.array(soil_params.loc[:,'loccode']) and lcode not in lcode_not_in):\n",
    "        lcode_not_in.append(lcode)\n",
    "        df_tests = df_tests[(df_tests['site_code'] != scode) & (df_tests['loccode'] != lcode)]\n",
    "\n",
    "# loop thru again to get necessary data\n",
    "for idx, row in df_tests.iterrows():\n",
    "    param = row['parameter']\n",
    "    scode = row['site_code']\n",
    "    lcode = row['loccode']\n",
    "    kocs += [float(pest_params[pest_params['parameter'] == param].loc[:,'KOC (m^3/Mg)=(cm^3/gr)'])]\n",
    "    apps += [float(pest_params[pest_params['parameter'] == param].loc[:,'apllayd amoint (ug/Hectar)'])]\n",
    "    oms += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Organic matter (%)'])]\n",
    "    bulks += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Bulk density (gr/cm3)'])]\n",
    "    sand += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Sand %'])]\n",
    "    silt += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Silt %'])]\n",
    "    clay += [float(soil_params[(soil_params['Identify'] == scode) & (soil_params['loccode'] == lcode)].loc[:,'Clay %'])]\n",
    "\n",
    "\n",
    "# append necessary data\n",
    "df_tests.loc[:,['koc [m^3/Mg]','application rates [mg/m^2]', 'organic matter [%]', 'bulk density [Mg/m^3]',\n",
    "                 'Sand %', 'Silt %', 'Clay %']] = np.array([kocs, apps, oms, bulks, sand, silt, clay]).T\n",
    "\n",
    "#adjust inorganic matter %\n",
    "for idx, row in df_tests.iterrows():\n",
    "    df_tests.loc[idx,'True Sand'] = round(row['Sand %'] / (100 + row['organic matter [%]']) * 100, 2)\n",
    "    df_tests.loc[idx,'True Silt'] = round(row['Silt %'] / (100 + row['organic matter [%]']) * 100, 2)\n",
    "    df_tests.loc[idx,'True Clay'] = round(row['Clay %'] / (100 + row['organic matter [%]']) * 100, 2)\n",
    "\n",
    "# convert necessary units for TGUS equation and isolate high bulk densities\n",
    "df_tests['application rates [mg/m^2]'] = df_tests['application rates [mg/m^2]'] * 0.0000001\n",
    "\n",
    "\n",
    "#df_tests = df_tests[df_tests['bulk density [Mg/m^3]'] >= 1] USE IF DOING MINERAL SIZES\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNO LONGER USING !!!!!!!!!\\n\\nIMPORTANT CONCEPT\\n\\n- theoretically, the organic carbon-water partition coefficient ('koc' column) and the organic carbon-water normalized Freundlich distribution \\n  coefficient will be treated as the same\\n\\n- this loop combines the columns, choosing koc first if it is available\\n\\n\\npcoef = []\\nfor idx, row in df_tests.iterrows():\\n    if row['koc'] > 0 :\\n        pcoef += [float(row['koc'])]\\n    else :\\n        pcoef += [float(row['kfoc'])]\\n\\ndf_tests['pcoef'] = pcoef\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NO LONGER USING !!!!!!!!!\n",
    "\n",
    "IMPORTANT CONCEPT\n",
    "\n",
    "- theoretically, the organic carbon-water partition coefficient ('koc' column) and the organic carbon-water normalized Freundlich distribution \n",
    "  coefficient will be treated as the same\n",
    "\n",
    "- this loop combines the columns, choosing koc first if it is available\n",
    "\n",
    "\n",
    "pcoef = []\n",
    "for idx, row in df_tests.iterrows():\n",
    "    if row['koc'] > 0 :\n",
    "        pcoef += [float(row['koc'])]\n",
    "    else :\n",
    "        pcoef += [float(row['kfoc'])]\n",
    "\n",
    "df_tests['pcoef'] = pcoef\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- extract all current columns of potential interest to be put into algorithms...NOT FINAl!!!!!!!!!!!!!!\n",
    "\n",
    "- other minor fixes\n",
    "'''\n",
    "col_list = ['private_name', 'loctype', 'aquifer_vulnerability', 'drainage_class', 'detlimit', 'sampdate', 'parameter','gus', \n",
    "            'soil_halflife', 'simphalflife', 'morehalflives', 'koc [m^3/Mg]','application rates [mg/m^2]', 'organic matter [%]', \n",
    "            'bulk density [Mg/m^3]', 'Sand %', 'True Sand', 'Silt %', 'True Silt', 'Clay %', 'True Clay', 'simpsorp', 'simpsorp2', 'result', 'simpresult']\n",
    "\n",
    "#get all columns of interest\n",
    "df_cols = df_tests.loc[:, col_list]\n",
    "\n",
    "#replace all instances of 'well drained' to 'Well drained'\n",
    "df_cols.replace(to_replace='well drained', value='Well drained', inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_adjusted = df_cols.reset_index().iloc[:,1:]\n",
    "\n",
    "# add a 'detected' column if result > 0\n",
    "# 1 if detected, -1 if not\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "    if df_adjusted.loc[idx, 'result'] > 0:\n",
    "        df_adjusted.loc[idx, 'detected'] = 1\n",
    "    else:\n",
    "        df_adjusted.loc[idx, 'detected'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTHIS IS NOT BEING DONE UNTIL FURTHER NOTICE!\\n\\nIMPORTANT CONCEPT:\\n\\n- at many testing sites, samples were taken in both the downgradient and upgradient groundwater of the pesticide-treated area...\\n  these are distinguished by 'Categorical - upgradient' and'Categorical - downgradient'...'Categorical - up and downgradient' indicates\\n  one site where the test was both upgradient of one treated area and downgradient of another\\n\\n- tests were done at upgradient sites to find out if pesticides were in the already in the groundwater NOT as a result of the land-owners'\\n  application...this could be the result of a neighboring property apply pesticides, for example...if the same pesticide is detected downgradient \\n  and upgradient of the pesticide application area, then the upgradient value should be subtracted from the downgradient value to get a better\\n  representation of what is happening with land-owners' pesticides\\n\\n- this loop identifies upgradient/downgradient tests on the same sampling date and subtracts the upgradient result from the downgradient\\n\\n\\n\\n# reset index\\ndf_reset = df_cols.reset_index().iloc[:,1:]\\n\\n\\n\\n\\nfor idx, row in df_reset.iterrows():\\n    # find 'upgradient' or 'up and downgradient' test on same date for same parameter in same location\\n    if row['loctype'] in ['Categorical - downgradient','Categorical - up and downgradient']:\\n        sampdate = row['sampdate']\\n        parameter = row['parameter']\\n        loctype = row['loctype']\\n        name = row['private_name']\\n        upgradient = df_reset[(df_reset['private_name'] == name) & (df_reset['sampdate'] == sampdate) & (df_reset['loctype'] > loctype) & (df_reset['parameter'] == parameter)]\\n\\n    # if test has both 'upgradient' and 'up and downgradient' samples, then subtract just the 'up and downgradient'\\n    # when upgradient is created, it puts 'up and downgradient' results first, so we can just subtract out first index of whatever upgradient is\\n    if len(upgradient) > 0:\\n      df_reset.loc[idx, 'result'] -= upgradient.loc[upgradient.index[0],'result']\\n\\n# now extract out just the downgradient tests of interests\\n# UNDECIDED ON THIS! for now just copy df_reset\\n#df_adjusted = df_reset[(df_reset['loctype'] != 'Categorical - upgradient') & (df_reset['loctype'] != 'Categorical - up and downgradient') ]\\n\\ndf_adjusted = df_reset.loc[:,:]\\n\\n\\n# add a 'detected' column if result > 0\\n# 1 if detected, -1 if not\\nfor idx, row in df_adjusted.iterrows():\\n    if df_adjusted.loc[idx, 'result'] > 0:\\n        df_adjusted.loc[idx, 'detected'] = 1\\n    else:\\n        df_adjusted.loc[idx, 'detected'] = -1\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "THIS IS NOT BEING DONE UNTIL FURTHER NOTICE!\n",
    "\n",
    "IMPORTANT CONCEPT:\n",
    "\n",
    "- at many testing sites, samples were taken in both the downgradient and upgradient groundwater of the pesticide-treated area...\n",
    "  these are distinguished by 'Categorical - upgradient' and'Categorical - downgradient'...'Categorical - up and downgradient' indicates\n",
    "  one site where the test was both upgradient of one treated area and downgradient of another\n",
    "\n",
    "- tests were done at upgradient sites to find out if pesticides were in the already in the groundwater NOT as a result of the land-owners'\n",
    "  application...this could be the result of a neighboring property apply pesticides, for example...if the same pesticide is detected downgradient \n",
    "  and upgradient of the pesticide application area, then the upgradient value should be subtracted from the downgradient value to get a better\n",
    "  representation of what is happening with land-owners' pesticides\n",
    "\n",
    "- this loop identifies upgradient/downgradient tests on the same sampling date and subtracts the upgradient result from the downgradient\n",
    "\n",
    "\n",
    "\n",
    "# reset index\n",
    "df_reset = df_cols.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in df_reset.iterrows():\n",
    "    # find 'upgradient' or 'up and downgradient' test on same date for same parameter in same location\n",
    "    if row['loctype'] in ['Categorical - downgradient','Categorical - up and downgradient']:\n",
    "        sampdate = row['sampdate']\n",
    "        parameter = row['parameter']\n",
    "        loctype = row['loctype']\n",
    "        name = row['private_name']\n",
    "        upgradient = df_reset[(df_reset['private_name'] == name) & (df_reset['sampdate'] == sampdate) & (df_reset['loctype'] > loctype) & (df_reset['parameter'] == parameter)]\n",
    "\n",
    "    # if test has both 'upgradient' and 'up and downgradient' samples, then subtract just the 'up and downgradient'\n",
    "    # when upgradient is created, it puts 'up and downgradient' results first, so we can just subtract out first index of whatever upgradient is\n",
    "    if len(upgradient) > 0:\n",
    "      df_reset.loc[idx, 'result'] -= upgradient.loc[upgradient.index[0],'result']\n",
    "\n",
    "# now extract out just the downgradient tests of interests\n",
    "# UNDECIDED ON THIS! for now just copy df_reset\n",
    "#df_adjusted = df_reset[(df_reset['loctype'] != 'Categorical - upgradient') & (df_reset['loctype'] != 'Categorical - up and downgradient') ]\n",
    "\n",
    "df_adjusted = df_reset.loc[:,:]\n",
    "\n",
    "\n",
    "# add a 'detected' column if result > 0\n",
    "# 1 if detected, -1 if not\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "    if df_adjusted.loc[idx, 'result'] > 0:\n",
    "        df_adjusted.loc[idx, 'detected'] = 1\n",
    "    else:\n",
    "        df_adjusted.loc[idx, 'detected'] = -1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY NEEDED IF LOCTYPE SUBTRACTION IS DONE\n",
    "# reset index again\n",
    "#df_adjusted = df_adjusted.reset_index().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TGUS EQUATION\n",
    "\n",
    "- SWL lab members are currently deriving a new theoretical groundwater ubiquity score (TGUS) to be compared to typically used groundwater ubiquity\n",
    "  score (GUS) derived by Gustafson et al., 1989\n",
    "\n",
    "- equation is written in 'funcs.py' file and called for each test\n",
    "\n",
    "'''\n",
    "for idx, row in df_adjusted.iterrows():\n",
    "  tgus = funcs.tgus(row['soil_halflife'], row['application rates [mg/m^2]'], row['detlimit'], row['organic matter [%]'],\n",
    "                    row['bulk density [Mg/m^3]'], row['koc [m^3/Mg]'])\n",
    "  df_adjusted.loc[idx, 'tgus'] = tgus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "FEATURE ENGINEERING:\n",
    "\n",
    "- creates final dataframe of all ordinal/one-hot data and raw data\n",
    "\n",
    "- recall: if doing SVM or something that isn't trees, need to do scale raw data\n",
    "'''\n",
    "# setup final dataframe\n",
    "# for now, working with all raw numbers and not pre-decided categories\n",
    "cat_cols = ['aquifer_vulnerability','drainage_class']\n",
    "raw_cols = ['gus','tgus','soil_halflife', 'koc [m^3/Mg]','detlimit', \n",
    "            'Sand %', 'Silt %', 'Clay %', 'True Sand', 'True Silt', 'True Clay']\n",
    "\n",
    "# normalize raw values ONLY IF NOT DOING TREES\n",
    "#norm = scaler.fit_transform(df_adjusted.loc[:, raw_cols])\n",
    "#norm = round(pd.DataFrame(norm, columns = raw_cols), 3)\n",
    "\n",
    "# ordinal encoder for categoricals\n",
    "df_cats= funcs.ordinal(df_adjusted, 'drainage_class')\n",
    "df_cats = funcs.ordinal(df_cats, 'aquifer_vulnerability')\n",
    "cats = df_cats.loc[:, cat_cols]\n",
    "'''\n",
    "# onehot categorical\n",
    "df_onehot = funcs.onehot(df=df_adjusted, columns = onehot_cols)\n",
    "df_final = pd.concat([df_onehot, norm], axis = 1)\n",
    "'''\n",
    "\n",
    "# combine, then append offset and re-add detected column\n",
    "df_final = pd.concat([cats, df_adjusted.loc[:, raw_cols]], axis = 1)\n",
    "df_final['offset'] = np.ones((df_adjusted.shape[0]))\n",
    "df_final['detected'] = df_adjusted['detected']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SEPARATE DATAFRAMES\n",
    "\n",
    "- 'df_final' is used to create different datasets to compare\n",
    "\n",
    "'''\n",
    "df_a = df_final.loc[:, ~df_final.columns.isin(['True Sand','True Silt', 'True Clay','Sand %', 'Silt %', 'Clay %'])]#'soil_halflife', 'koc [m^3/Mg]','detlimit'])]\n",
    "df_b = df_final.loc[:, ~df_final.columns.isin(['True Sand','True Silt', 'True Clay','tgus','soil_halflife', 'koc [m^3/Mg]', 'Sand %', 'Silt %', 'Clay %' ])]# 'aquifer_vulnerability', 'drainage_class','detlimit'\n",
    "df_c = df_final.loc[:, ~df_final.columns.isin(['True Sand','True Silt', 'True Clay','gus','soil_halflife', 'koc [m^3/Mg]',  'Sand %', 'Clay %', 'Silt %'])]#, 'aquifer_vulnerability', 'drainage_class','detlimit'\n",
    "#df_raw = df_final.loc[:, ~df_final.columns.isin(['tgus', 'tgus*', 'gus','detlimit'])]\n",
    "df_d = df_final.loc[:, ~df_final.columns.isin(['gus','tgus','True Sand','True Silt', 'True Clay', 'Sand %', 'Silt %', 'Clay %'])]#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write df_final as csv for future use\n",
    "df_a.to_csv(path_or_buf = '../data/df_a.csv', sep = ',')\n",
    "df_b.to_csv(path_or_buf = '../data/df_b.csv', sep = ',')\n",
    "df_c.to_csv(path_or_buf = '../data/df_c.csv', sep = ',')\n",
    "#df_raw.to_csv(path_or_buf = '../data/df_raw.csv', sep = ',')\n",
    "df_d.to_csv(path_or_buf = '../data/df_d.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
